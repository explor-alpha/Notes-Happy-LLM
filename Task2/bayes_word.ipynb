{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.实验内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验对朴素贝叶斯算法原理展开介绍，基于朴素贝叶斯算法完成言论过滤器实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.实验目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过本实验掌握朴素贝叶斯算法原理，了解朴素贝叶斯算法如何应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 实验知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 朴素贝叶斯算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 实验环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* python 3.6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.预备知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 概率论与数理统计\n",
    "* Linux命令基本操作\n",
    "* Python编程基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯算法是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题。该算法的优点在于简单易懂、学习效率高、在某些领域的分类问题中能够与决策树、神经网络相媲美。但由于该算法以自变量之间的独立（条件特征独立）性和连续变量的正态性假设为前提，就会导致算法精度在某种程度上受影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验首先从朴素贝叶斯推断原理开始学习朴素贝叶斯算法，然后使用一个简单的例子演示该算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯是贝叶斯决策理论的一部分，所以在讲述朴素贝叶斯之前有必要快速了解一下贝叶斯决策理论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贝叶斯决策理论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设现在我们有一个数据集，它由两类数据组成，数据分布如下图所示：  \n",
    "![](1_bayes_word.jpg)  \n",
    "我们现在用p1(x,y)表示数据点(x,y)属于类别1(图中红色圆点表示的类别)的概率，用p2(x,y)表示数据点(x,y)属于类别2(图中蓝色三角形表示的类别)的概率，那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：  \n",
    "\n",
    "* 如果p1(x,y) > p2(x,y)，那么类别为1\n",
    "* 如果p1(x,y) < p2(x,y)，那么类别为2  \n",
    "\n",
    "也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。已经了解了贝叶斯决策理论的核心思想，那么接下来，就是学习如何计算p1和p2概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 条件概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在学习计算p1和p2概率之前，我们需要了解什么是条件概率(Condittional probability)，就是指在事件B发生的情况下，事件A发生的概率，用P(A|B)来表示。  \n",
    "![](2_bayes_word.jpg)  \n",
    "根据文氏图，可以很清楚地看到在事件B发生的情况下，事件A发生的概率就是P(A∩B)除以P(B)。  \n",
    "![](3_bayes_word.jpg)  \n",
    "因此，  \n",
    "![](4_bayes_word.jpg)  \n",
    "同理可得，  \n",
    "![](5_bayes_word.jpg)  \n",
    "所以，  \n",
    "![](6_bayes_word.jpg)  \n",
    "即，  \n",
    "![](7_bayes_word.png)  \n",
    "这就是条件概率的计算公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全概率公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了条件概率以外，在计算p1和p2的时候，还要用到全概率公式，因此，这里继续推导全概率公式。\n",
    "\n",
    "假定样本空间S，是两个事件A与A’的和。  \n",
    "![](8_bayes_word.jpg)  \n",
    "上图中，红色部分是事件A，绿色部分是事件A’，它们共同构成了样本空间S。\n",
    "\n",
    "在这种情况下，事件B可以划分成两个部分。  \n",
    "![](9_bayes_word.jpg)  \n",
    "即  \n",
    "![](10_bayes_word.jpg)  \n",
    "在上一节的推导当中，我们已知  \n",
    "![](11_bayes_word.jpg)  \n",
    "所以，  \n",
    "![](12_bayes_word.jpg)  \n",
    "这就是全概率公式。它的含义是，如果A和A’构成样本空间的一个划分，那么事件B的概率，就等于A和A’的概率分别乘以B对这两个事件的条件概率之和。\n",
    "\n",
    "将这个公式代入上一节的条件概率公式，就得到了条件概率的另一种写法：\n",
    "![](13_bayes_word.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贝叶斯推断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对条件概率公式进行变形，可以得到如下形式：  \n",
    "![](14_bayes_word.png)  \n",
    "我们把P(A)称为”先验概率”（Prior probability），即在B事件发生之前，我们对A事件概率的一个判断。  \n",
    "P(A|B)称为”后验概率”（Posterior probability），即在B事件发生之后，我们对A事件概率的重新评估。  \n",
    "P(B|A)/P(B)称为”可能性函数”（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。 \n",
    "所以，条件概率可以理解成下面的式子：  \n",
    "　　后验概率　＝　先验概率 ｘ 调整因子1  \n",
    "这就是贝叶斯推断的含义。我们先预估一个”先验概率”，然后加入实验结果，看这个实验到底是增强还是削弱了”先验概率”，由此得到更接近事实的”后验概率”。  \n",
    "在这里，如果”可能性函数”P(B|A)/P(B)>1，意味着”先验概率”被增强，事件A的发生的可能性变大；如果”可能性函数”=1，意味着B事件无助于判断事件A的可能性；如果”可能性函数”<1，意味着”先验概率”被削弱，事件A的可能性变小。  \n",
    "为了加深对贝叶斯推断的理解，我们一个例子。  \n",
    "![](15_bayes_word.jpg)  \n",
    "两个一模一样的碗，一号碗有30颗水果糖和10颗巧克力糖，二号碗有水果糖和巧克力糖各20颗。现在随机选择一个碗，从中摸出一颗糖，发现是水果糖。请问这颗水果糖来自一号碗的概率有多大？  \n",
    "我们假定，H1表示一号碗，H2表示二号碗。由于这两个碗是一样的，所以P(H1)=P(H2)，也就是说，在取出水果糖之前，这两个碗被选中的概率相同。因此，P(H1)=0.5，我们把这个概率就叫做”先验概率”，即没有做实验之前，来自一号碗的概率是0.5。  \n",
    "再假定，E表示水果糖，所以问题就变成了在已知E的情况下，来自一号碗的概率有多大，即求P(H1|E)。我们把这个概率叫做”后验概率”，即在E事件发生之后，对P(H1)的修正。  \n",
    "根据条件概率公式，得到  \n",
    "![](16_bayes_word.jpg)  \n",
    "已知，P(H1)等于0.5，P(E|H1)为一号碗中取出水果糖的概率，等于30÷(30+10)=0.75，那么求出P(E)就可以得到答案。根据全概率公式，  \n",
    "![](17_bayes_word.jpg)  \n",
    "所以，  \n",
    "![](18_bayes_word.jpg)  \n",
    "将数字代入原方程，得到  \n",
    "![](19_bayes_word.jpg)  \n",
    "这表明，来自一号碗的概率是0.6。也就是说，取出水果糖之后，H1事件的可能性得到了增强。\n",
    "同时再思考一个问题，在使用该算法的时候，如果不需要知道具体的类别概率，即上面P(H1|E)=0.6，只需要知道所属类别，即来自一号碗，我们有必要计算P(E)这个全概率吗？要知道我们只需要比较 P(H1|E)和P(H2|E)的大小，找到那个最大的概率就可以。既然如此，两者的分母都是相同的，那我们只需要比较分子即可。即比较P(E|H1)P(H1)和P(E|H2)P(H2)的大小，所以为了减少计算量，全概率公式在实际编程中可以不使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯推断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解了贝叶斯推断，那么让我们继续看看朴素贝叶斯。贝叶斯和朴素贝叶斯的概念是不同的，区别就在于“朴素”二字，朴素贝叶斯对条件个概率分布做了条件独立性的假设。 比如下面的公式，假设有n个特征：  \n",
    "![](20_bayes_word.jpg)  \n",
    "由于每个特征都是独立的，我们可以进一步拆分公式  \n",
    "![](21_bayes_word.jpg)  \n",
    "这样我们就可以进行计算了。如果有些迷糊，让我们从一个例子开始讲起，你会看到贝叶斯分类器很好懂，一点都不难。  \n",
    "某个医院早上来了六个门诊的病人，他们的情况如下表所示：  \n",
    "![](22_bayes_word.png)  \n",
    "现在又来了第七个病人，是一个打喷嚏的建筑工人。请问他患上感冒的概率有多大？  \n",
    "根据贝叶斯定理：  \n",
    "![](23_bayes_word.png)  \n",
    "可得：  \n",
    "![](24_bayes_word.png)  \n",
    "根据朴素贝叶斯条件独立性的假设可知，”打喷嚏”和”建筑工人”这两个特征是独立的，因此，上面的等式就变成了  \n",
    "![](25_bayes_word.jpg)  \n",
    "这里可以计算：  \n",
    "![](26_bayes_word.jpg)  \n",
    "因此，这个打喷嚏的建筑工人，有66%的概率是得了感冒。同理，可以计算这个病人患上过敏或脑震荡的概率。比较这几个概率，就可以知道他最可能得什么病。  这就是贝叶斯分类器的基本方法：在统计资料的基础上，依据某些特征，计算各个类别的概率，从而实现分类。  \n",
    "同样，在编程的时候，如果不需要求出所属类别的具体概率，P(打喷嚏) = 0.5和P(建筑工人) = 0.33的概率是可以不用求的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ！！！实验-言论过滤器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以在线社区留言为例。为了不影响社区的发展，我们要屏蔽侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标志为内容不当。过滤这类内容是一个很常见的需求。对此问题建立两个类型：侮辱类和非侮辱类，使用1和0分别表示。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1. 预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词条切分和词性标注 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑出现所有文档中的单词，再决定将哪些单词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。简单起见，我们先假设已经将本文切分完毕，存放到列表中，并对词汇向量进行分类标注。编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid']\n",
      "['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him']\n",
      "['stop', 'posting', 'stupid', 'worthless', 'garbage']\n",
      "['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him']\n",
      "['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
      "[0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\"\"\"\n",
    "函数说明:创建实验样本\n",
    "Parameters:\n",
    "    无\n",
    "Returns:\n",
    "    postingList - 实验样本切分的词条\n",
    "    classVec - 类别标签向量\n",
    "\"\"\"\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],   #切分的词条\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]      #类别标签向量，1代表侮辱性词汇，0代表不是\n",
    "    return postingList,classVec\n",
    "if __name__ == '__main__':\n",
    "    postingLIst, classVec = loadDataSet()    \n",
    "    for each in postingLIst:\n",
    "        print(each)\n",
    "    print(classVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从运行结果可以看出，我们已经将postingList是存放词条列表中，classVec是存放每个词条的所属类别，1代表侮辱类 ，0代表非侮辱类。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成词条向量 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继续编写代码，前面我们已经说过我们要先创建一个词汇表，并将切分好的词条转换为词条向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postingList:\n",
      " [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "myVocabList:\n",
      " ['please', 'steak', 'dalmation', 'to', 'I', 'worthless', 'mr', 'how', 'take', 'cute', 'ate', 'love', 'problems', 'help', 'food', 'licks', 'quit', 'stupid', 'dog', 'park', 'is', 'garbage', 'maybe', 'flea', 'so', 'posting', 'stop', 'buying', 'him', 'has', 'my', 'not']\n",
      "trainMat:\n",
      " [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "def createVocabList(dataSet):\n",
    "    \"\"\"\n",
    "    函数说明:将切分的实验样本词条整理成不重复的词条列表，也就是词汇表\n",
    "\n",
    "    Parameters:\n",
    "        dataSet - 整理的样本数据集\n",
    "    Returns:\n",
    "        vocabSet - 返回不重复的词条列表，也就是词汇表\n",
    "    \"\"\"\n",
    "    vocabSet = set([])                      #创建一个空的不重复列表\n",
    "    for document in dataSet:               \n",
    "        vocabSet = vocabSet | set(document) #取并集\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"\n",
    "    函数说明:根据vocabList词汇表，将inputSet向量化，向量的每个元素为1或0。\n",
    "    for 每个inputSet列表， 创建一个长度与词汇表相同的向量，每个元素的值（0或1）表示词汇表中的值是否在inputSet中出现\n",
    "    \n",
    "    Parameters:\n",
    "        vocabList - createVocabList返回的列表\n",
    "        inputSet - 切分的词条列表\n",
    "    Returns:\n",
    "        returnVec - 文档向量,词集模型\n",
    "    \"\"\"\n",
    "    returnVec = [0] * len(vocabList)                                    #创建一个其中所含元素都为0的向量\n",
    "    for word in inputSet:                                                #遍历每个词条\n",
    "        if word in vocabList:                                            #如果词条存在于词汇表中，则置1\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)    \n",
    "    return returnVec #返回文档向量\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    postingList, classVec = loadDataSet()\n",
    "    print('postingList:\\n',postingList)\n",
    "    myVocabList = createVocabList(postingList)\n",
    "    print('myVocabList:\\n',myVocabList)\n",
    "    trainMat = []    \n",
    "    for postinDoc in postingList:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    print('trainMat:\\n', trainMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从运行结果可以看出：\n",
    "- postingList是原始的词条列表，classVec类别标签\n",
    "- myVocabList是词汇表。\n",
    "    - myVocabList是所有单词出现的集合，没有重复的元素。词汇表是用来干什么的？没错，它是用来将词条向量化的，一个单词在词汇表中出现过一次，那么就在相应位置记作1，如果没有出现就在相应位置记作0。\n",
    "- trainMat是所有的词条向量组成的列表。它里面存放的是根据myVocabList**向量化（字符转化数字）**的词条向量。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part2. 【练习】训练朴素贝叶斯分类器 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经得到了词条向量。接下来，我们就可以通过词条向量训练朴素贝叶斯分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myVocabList:\n",
      " ['please', 'steak', 'dalmation', 'to', 'I', 'worthless', 'mr', 'how', 'take', 'cute', 'ate', 'love', 'problems', 'help', 'food', 'licks', 'quit', 'stupid', 'dog', 'park', 'is', 'garbage', 'maybe', 'flea', 'so', 'posting', 'stop', 'buying', 'him', 'has', 'my', 'not']\n",
      "p0V:\n",
      " [0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.\n",
      " 0.04166667 0.04166667 0.         0.04166667 0.04166667 0.04166667\n",
      " 0.04166667 0.04166667 0.         0.04166667 0.         0.\n",
      " 0.04166667 0.         0.04166667 0.         0.         0.04166667\n",
      " 0.04166667 0.         0.04166667 0.         0.08333333 0.04166667\n",
      " 0.125      0.        ]\n",
      "p1V:\n",
      " [0.         0.         0.         0.05263158 0.         0.10526316\n",
      " 0.         0.         0.05263158 0.         0.         0.\n",
      " 0.         0.         0.05263158 0.         0.05263158 0.15789474\n",
      " 0.10526316 0.05263158 0.         0.05263158 0.05263158 0.\n",
      " 0.         0.05263158 0.05263158 0.05263158 0.05263158 0.\n",
      " 0.         0.05263158]\n",
      "classVec:\n",
      " [0, 1, 0, 1, 0, 1]\n",
      "pAb:\n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\"\"\"\n",
    "    提示：\n",
    "    #计算训练的文档数目\n",
    "    \n",
    "    #计算每篇文档的词条数 \n",
    "    \n",
    "    #文档属于侮辱类的概率\n",
    "    \n",
    "    #创建numpy.zeros数组\n",
    "    \n",
    "    #分母初始化为0.0\n",
    "    \n",
    "    \n",
    "        #统计属于侮辱类的条件概率所需的数据，即P(w0|1),P(w1|1),P(w2|1)···\n",
    "        \n",
    "        #统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)···\n",
    "        \n",
    "    #相除 \n",
    "    \n",
    "    #返回属于侮辱类的条件概率数组，属于非侮辱类的条件概率数组，文档属于侮辱类的概率\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    \"\"\"\n",
    "    函数说明:朴素贝叶斯分类器训练函数\n",
    "\n",
    "    Parameters:\n",
    "        trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵\n",
    "        trainCategory - 训练类别标签向量，即loadDataSet返回的classVec\n",
    "    Returns:\n",
    "        p0Vect - 非侮辱类的条件概率数组\n",
    "        p1Vect - 侮辱类的条件概率数组\n",
    "        pAbusive - 文档属于侮辱类的概率\n",
    "    \"\"\"\n",
    "\n",
    "    ### 1. 先验概率： P(侮辱类)\n",
    "    # 1.1. 得到训练文档的总数和词汇表中单词的个数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    \n",
    "    # 1.2. 计算文档属于侮辱类别的概率 p(侮辱)\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    \n",
    "    ### 2. 密度： P(xi|非侮辱类) & P(xi|侮辱类)\n",
    "    ''' \n",
    "    参数说明——P(xi|非侮辱类) & P(xi|侮辱类)：\n",
    "        - ！：依赖与词汇表中各单词的位置\n",
    "        - 数据形式：都为与词汇表等长的array\n",
    "\n",
    "        - 命名：piVect        \n",
    "        - xi：每个单词\n",
    "\n",
    "        - 计算式：piVect = piNum / piDenom\n",
    "            - piNum：与词汇表等长的array。用于统计在非侮辱类or侮辱类句子中，每个xi出现的次数\n",
    "            - piDenom：一个数值。piNum的和\n",
    "    '''    \n",
    "    # 2.1. 初始化 piNum\n",
    "    p0Num = np.zeros(numWords)  # 类别为0（非侮辱）的词条出现数\n",
    "    p1Num = np.zeros(numWords)  # 类别为1（侮辱）的词条出现数\n",
    "    \n",
    "    # 2.2. 初始化 piDenom\n",
    "    p0Denom = 0  # 类别为0的总词数\n",
    "    p1Denom = 0  # 类别为1的总词数\n",
    "\n",
    "    # 2.3. 遍历每个文档，统计各类别下各词出现的次数\n",
    "    for i in range(len(trainMatrix)): # 经历所有6个句子\n",
    "        if trainCategory[i] == 1: # 标记为侮辱性的句子\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    \n",
    "    # 2.4. 计算条件概率：每个词在对应类别下出现的概率，并取对数防止下溢\n",
    "    p1Vect = p1Num / p1Denom # 侮辱类别的条件概率向量\n",
    "    p0Vect = p0Num / p0Denom # 非侮辱类别的条件概率向量\n",
    "\n",
    "    return p0Vect, p1Vect, pAbusive    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    postingList, classVec = loadDataSet()\n",
    "    myVocabList = createVocabList(postingList)\n",
    "    print('myVocabList:\\n', myVocabList)\n",
    "    trainMat = []    \n",
    "    for postinDoc in postingList:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "        \n",
    "    p0V, p1V, pAb = trainNB0(trainMat, classVec)\n",
    "    print('p0V:\\n', p0V)\n",
    "    print('p1V:\\n', p1V)\n",
    "    print('classVec:\\n', classVec)\n",
    "    print('pAb:\\n', pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行结果如上所示：\n",
    "- p0V存放的是每个单词属于类别0，也就是非侮辱类词汇的概率。比如p0V的倒数第2个概率，就是stupid这个单词属于非侮辱类的概率为0。\n",
    "- 同理，p1V的倒数第2个概率，就是stupid这个单词属于侮辱类的概率为0.15789474，也就是约等于15.79%的概率。我们知道stupid的中文意思是蠢货，显而易见，这个单词属于侮辱类。\n",
    "- pAb是所有侮辱类的样本占所有样本的概率，从classVec中可以看出，一用有3个侮辱类，3个非侮辱类。所以侮辱类的概率是0.5。\n",
    "\n",
    "---\n",
    "\n",
    "- p0V——如P(him|非侮辱类) = 0.0833、P(is|非侮辱类) = 0.0417，一直到P(dog|非侮辱类) = 0.0417，各单词属于非侮辱类的条件概率。\n",
    "- p1V——各个单词属于侮辱类的条件概率。\n",
    "- pAb——先验概率：P(侮辱类)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part3. 使用分类器进行分类 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上个步骤已经训练好分类器，接下来，使用分类器进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0: 0.0\n",
      "p1: 0.0\n",
      "['love', 'my', 'dalmation'] 属于非侮辱类\n",
      "p0: 0.0\n",
      "p1: 0.0\n",
      "['stupid', 'garbage'] 属于非侮辱类\n"
     ]
    }
   ],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    \"\"\"\n",
    "    函数说明:朴素贝叶斯分类器分类函数\n",
    "\n",
    "    Parameters:\n",
    "        vec2Classify - 待分类的词条数组\n",
    "        p0Vec - 非侮辱类的条件概率数组\n",
    "        p1Vec -侮辱类的条件概率数组\n",
    "        pClass1 - 文档属于侮辱类的概率\n",
    "    Returns:\n",
    "        0 - 属于非侮辱类\n",
    "        1 - 属于侮辱类\n",
    "    \"\"\"\n",
    "    p1 = reduce(lambda x,y:x*y, vec2Classify * p1Vec) * pClass1  #对应元素相乘\n",
    "    p0 = reduce(lambda x,y:x*y, vec2Classify * p0Vec) * (1.0 - pClass1)\n",
    "    print('p0:',p0)\n",
    "    print('p1:',p1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "\n",
    "def testingNB():\n",
    "    \"\"\"\n",
    "    函数说明:测试朴素贝叶斯分类器\n",
    "\n",
    "    Parameters:\n",
    "    无\n",
    "    Returns:\n",
    "    无\n",
    "    \"\"\"\n",
    "    listOPosts,listClasses = loadDataSet()\t#创建实验样本\n",
    "    myVocabList = createVocabList(listOPosts)\t#创建词汇表\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\t#将实验样本向量化\n",
    "    p0V,p1V,pAb = trainNB0(np.array(trainMat),np.array(listClasses))\t#训练朴素贝叶斯分类器\n",
    "    \n",
    "    testEntry = ['love', 'my', 'dalmation']\t\t#测试样本1\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\t#测试样本向量化\n",
    "    if classifyNB(thisDoc,p0V,p1V,pAb):\n",
    "        print(testEntry,'属于侮辱类')\t\t#执行分类并打印分类结果\n",
    "    else:\n",
    "        print(testEntry,'属于非侮辱类')\t\t#执行分类并打印分类结果\n",
    "    testEntry = ['stupid', 'garbage']\t\t#测试样本2\n",
    "\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\t#测试样本向量化\n",
    "    if classifyNB(thisDoc,p0V,p1V,pAb):\n",
    "        print(testEntry,'属于侮辱类')\t\t#执行分类并打印分类结果\n",
    "    else:\n",
    "        print(testEntry,'属于非侮辱类')\t\t#执行分类并打印分类结果\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们测试了两个词条，在使用分类器前，也需要对词条向量化，然后使用classifyNB()函数，用朴素贝叶斯公式，计算词条向量属于侮辱类和非侮辱类的概率。你会发现，这样写的算法无法进行分类，p0和p1的计算结果都是0，这里显然存在问题。这是为什么呢？下个实验步骤我们继续解决这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part4. 算法改进 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算p(w0|1)p(w1|1)p(w2|1)。如果其中有一个概率值为0，那么最后的成绩也为0。我们拿出上一个步骤中的截图。  \n",
    "![](27_bayes_word.png)  \n",
    "从上图可以看出，在计算的时候已经出现了概率为0的情况。如果新实例文本，包含这种概率为0的分词，那么最终的文本属于某个类别的概率也就是0了。显然，这样是不合理的，为了降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。这种做法就叫做拉普拉斯平滑(Laplace Smoothing)又被称为加1平滑，是比较常用的平滑方法，它就是为了解决0概率问题。  \n",
    "除此之外，另外一个遇到的问题就是下溢出，这是由于太多很小的数相乘造成的。我们知道，两个小数相乘，越乘越小，这样就造成了下溢出。在程序中，在相应小数位置进行四舍五入，计算结果可能就变成0了。为了解决这个问题，对乘积结果取自然对数。通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。下图给出函数f(x)和ln(f(x))的曲线。  \n",
    "![](28_bayes_word.jpg)  \n",
    "检查这两条曲线，就会发现它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。因此我们可以对trainNB0(trainMatrix, trainCategory)函数进行更改，修改如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myVocabList:\n",
      " ['please', 'steak', 'dalmation', 'to', 'I', 'worthless', 'mr', 'how', 'take', 'cute', 'ate', 'love', 'problems', 'help', 'food', 'licks', 'quit', 'stupid', 'dog', 'park', 'is', 'garbage', 'maybe', 'flea', 'so', 'posting', 'stop', 'buying', 'him', 'has', 'my', 'not']\n",
      "p0V:\n",
      " [-2.56494936 -2.56494936 -2.56494936 -2.56494936 -2.56494936 -3.25809654\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -3.25809654 -3.25809654\n",
      " -2.56494936 -3.25809654 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.15948425 -2.56494936\n",
      " -1.87180218 -3.25809654]\n",
      "p1V:\n",
      " [-3.04452244 -3.04452244 -3.04452244 -2.35137526 -3.04452244 -1.94591015\n",
      " -3.04452244 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -3.04452244\n",
      " -3.04452244 -3.04452244 -2.35137526 -3.04452244 -2.35137526 -1.65822808\n",
      " -1.94591015 -2.35137526 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -3.04452244 -2.35137526 -2.35137526 -2.35137526 -2.35137526 -3.04452244\n",
      " -3.04452244 -2.35137526]\n",
      "classVec:\n",
      " [0, 1, 0, 1, 0, 1]\n",
      "pAb:\n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    提示：\n",
    "    \n",
    "    #计算训练的文档数目\n",
    "    \n",
    "    #计算每篇文档的词条数\n",
    "    \n",
    "    #文档属于侮辱类的概率\n",
    "    \n",
    "    #创建numpy.ones数组,词条出现数初始化为1，拉普拉斯平滑\n",
    "    \n",
    "    #分母初始化为2,拉普拉斯平滑\n",
    "    \n",
    "    \n",
    "        #统计属于侮辱类的条件概率所需的数据，即P(w0|1),P(w1|1),P(w2|1)\n",
    "        \n",
    "        #统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)\n",
    "        \n",
    "    #取对数，防止下溢出\n",
    "    \n",
    "    #返回属于侮辱类的条件概率数组，属于非侮辱类的条件概率数组，文档属于侮辱类的概率\n",
    "\"\"\"\n",
    "\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    \"\"\"\n",
    "    函数说明:朴素贝叶斯分类器训练函数\n",
    "    Parameters:\n",
    "        trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵\n",
    "        trainCategory - 训练类别标签向量，即loadDataSet返回的classVec\n",
    "    Returns:\n",
    "        p0Vect - 非侮辱类的条件概率数组\n",
    "        p1Vect - 侮辱类的条件概率数组\n",
    "        pAbusive - 文档属于侮辱类的概率\n",
    "    \"\"\"\n",
    "\n",
    "    ### 1. 先验概率： P(侮辱类)\n",
    "    # 1.1. 得到训练文档的总数和词汇表中单词的个数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    \n",
    "    # 1.2. 计算文档属于侮辱类别的概率 p(侮辱)\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    \n",
    "    ### 2. 密度： P(xi|非侮辱类) & P(xi|侮辱类)\n",
    "    ''' \n",
    "    参数说明——P(xi|非侮辱类) & P(xi|侮辱类)：\n",
    "        - ！：依赖与词汇表中各单词的位置\n",
    "        - 数据形式：都为与词汇表等长的array\n",
    "\n",
    "        - 命名：piVect        \n",
    "        - xi：每个单词\n",
    "\n",
    "        - 计算式：piVect = piNum / piDenom\n",
    "            - piNum：与词汇表等长的array。用于统计在非侮辱类or侮辱类句子中，每个xi出现的次数\n",
    "            - piDenom：一个数值。piNum的和\n",
    "    '''    \n",
    "    # 2.1. 初始化 piNum\n",
    "    p0Num = np.ones(numWords)  # 类别为0（非侮辱）的词条出现数。      #创建numpy.ones数组,词条出现数初始化为1，拉普拉斯平滑\n",
    "    p1Num = np.ones(numWords)  # 类别为1（侮辱）的词条出现数。   #创建numpy.ones数组,词条出现数初始化为1，拉普拉斯平滑\n",
    "    \n",
    "    # 2.2. 初始化 piDenom\n",
    "    p0Denom = 2  # 类别为0的总词数。     #分母初始化为2,拉普拉斯平滑\n",
    "    p1Denom = 2  # 类别为1的总词数。     #分母初始化为2,拉普拉斯平滑\n",
    "\n",
    "    # 2.3. 遍历每个文档，统计各类别下各词出现的次数\n",
    "    for i in range(len(trainMatrix)): # 经历所有6个句子\n",
    "        if trainCategory[i] == 1: # 标记为侮辱性的句子\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    \n",
    "    # 2.4. 计算条件概率：每个词在对应类别下出现的概率，并取对数防止下溢\n",
    "    p1Vect = np.log(p1Num / p1Denom) # 侮辱类别的条件概率向量    #取对数，防止下溢出\n",
    "    p0Vect = np.log(p0Num / p0Denom) # 非侮辱类别的条件概率向量    #取对数，防止下溢出\n",
    "\n",
    "    return p0Vect, p1Vect, pAbusive        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    postingList, classVec = loadDataSet()\n",
    "    myVocabList = createVocabList(postingList)\n",
    "    print('myVocabList:\\n', myVocabList)\n",
    "    trainMat = []    \n",
    "    for postinDoc in postingList:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V, p1V, pAb = trainNB0(trainMat, classVec)\n",
    "    print('p0V:\\n', p0V)\n",
    "    print('p1V:\\n', p1V)\n",
    "    print('classVec:\\n', classVec)\n",
    "    print('pAb:\\n', pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样得到的结果就没有问题了，不存在0概率。此外，还需要修改classifyNB(vec2Classify, p0Vec, p1Vec, pClass1)函数，修改如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] 属于非侮辱类\n",
      "['stupid', 'garbage'] 属于侮辱类\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "函数说明:朴素贝叶斯分类器分类函数\n",
    "Parameters:\n",
    "    vec2Classify - 待分类的词条数组\n",
    "    p0Vec - 非侮辱类的条件概率数组\n",
    "    p1Vec -侮辱类的条件概率数组\n",
    "    pClass1 - 文档属于侮辱类的概率\n",
    "Returns:\n",
    "    0 - 属于非侮辱类\n",
    "    1 - 属于侮辱类\n",
    "\"\"\"\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)        #对应元素相乘。logA * B = logA + logB，所以这里加上log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)    \n",
    "    if p1 > p0:        \n",
    "        return 1\n",
    "    else:        \n",
    "        return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttestingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样修改的原因是由于取自然对数，所以需要将惩罚换成加法，原理是log(ab) = loga + logb。至此，我们的朴素贝叶斯分类器就改进完毕了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过本实验，您应该能达到以下两个目标：\n",
    "* 1. 掌握朴素贝叶斯算法原理。\n",
    "* 2. 熟悉朴素贝叶斯算法的初步应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献及延伸阅读"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考资料："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.哈林顿，李锐. 机器学习实战 : Machine learning in action[M]. 人民邮电出版社, 2013.\n",
    "* 2.周志华. 机器学习:Machine learning[M]. 清华大学出版社, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 延伸阅读："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.李航. 统计学习方法[M]. 清华大学出版社, 2012."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
